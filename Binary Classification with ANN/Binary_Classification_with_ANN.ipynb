{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yavg6Dn9wYlm"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Dataset for Binary Classification\n",
        "\n",
        "This dataset was created to build a binary classification model for predicting the likelihood of individuals experiencing side effects due to a certain treatment. The dataset is synthetically generated for educational purposes.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "- **Data Type**: Synthetically generated\n",
        "- **Problem Type**: Binary Classification\n",
        "- **Classes**: Side Effect (1), No Side Effect (0)\n",
        "\n",
        "## Features\n",
        "\n",
        "The dataset contains one feature:\n",
        "\n",
        "- **Age**: The age of individuals\n",
        "\n",
        "## Labels\n",
        "\n",
        "- **Side Effect (1)**: Individuals who experienced side effects.\n",
        "- **No Side Effect (0)**: Individuals who did not experience side effects.\n",
        "\n",
        "## Data Split\n",
        "\n",
        "- Training Data: 50% younger individuals with side effects, 50% older individuals without side effects, and 95% younger and older individuals with no side effects.\n",
        "- Testing Data: Custom data for prediction.\n",
        "\n",
        "The goal is to train a model to predict whether an individual is likely to experience side effects based on their age.\n",
        "\n",
        "## Sample Data\n",
        "\n",
        "Here is a sample of the data:\n",
        "\n",
        "| Age | Label |\n",
        "|-----|-------|\n",
        "| 30  |   0   |\n",
        "| 54  |   1   |\n",
        "| 100 |   0   |\n",
        "| ... |  ...  |\n",
        "\n",
        "## Data Scaling\n",
        "\n",
        "The data was scaled to a range of [0, 1] to ensure that different features are on a similar scale.\n",
        "\n",
        "The model used for classification is a neural network with multiple dense layers.\n",
        "\n",
        "Please note that this dataset is for educational purposes and does not represent real-world data.\n",
        "\n",
        "---\n",
        "\n",
        "Feel free to modify this Markdown cell to add more details or explanations as needed. This will help anyone reading your notebook understand the problem and data you're working with."
      ],
      "metadata": {
        "id": "zl7m67hfxtTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = []\n",
        "train_label = []\n"
      ],
      "metadata": {
        "id": "N5xII47le5vS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "  # 5% of younger individuals who did expereince a side effect\n",
        "  random_younger = randint(13, 64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_label.append(1) # 1 means side effects\n",
        "\n",
        "  # 5% of older individuals who did not experience a side effect\n",
        "  random_older = randint(65, 100)\n",
        "  train_samples.append(random_older)\n",
        "  train_label.append(0) # 0 means no side effects\n",
        "\n",
        "for i in range(1000):\n",
        "  # 95% of younger individuals that did not experience any side effects\n",
        "  random_younger = randint(13, 64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_label.append(0)\n",
        "\n",
        "  # 95% of older individuals that did experience side effects\n",
        "  random_older = randint(65, 100)\n",
        "  train_samples.append(random_older)\n",
        "  train_label.append(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "UFJhDzj3fGA8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our list to ndim array using numpy\n",
        "train_samples = np.array(train_samples)\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "# shuffle the data to add randomnes\n",
        "train_label, train_samples = shuffle(train_label, train_samples)"
      ],
      "metadata": {
        "id": "oUiYrevNfQ2B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing our data,  Normalization scales the data to a specific range,\n",
        "# typically between 0 and 1, to ensure that different features are on a similar\n",
        "# scale. This can improve the training process\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "\n",
        "# because the train_sample is a 1D numpy array of ages\n",
        "# and we convert it into 2D array where each age is its own row, the -1 is a place\n",
        "# holder that tells the numpy to calculate the number of rows automatically\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1, 1))\n",
        "scaled_train_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVwPvusof5JH",
        "outputId": "f6db8721-10c8-4378-d534-8b3eb5e2d60c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.31034483],\n",
              "       [0.88505747],\n",
              "       [0.68965517],\n",
              "       ...,\n",
              "       [0.03448276],\n",
              "       [0.59770115],\n",
              "       [0.62068966]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a simple tf.keras Sequential Model**"
      ],
      "metadata": {
        "id": "qb1ws2XRgvBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy"
      ],
      "metadata": {
        "id": "str7ARdyhWX8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5w4NwTSCgtgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the sequential class\n",
        "model = Sequential([\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'), # 1st hidden layer\n",
        "    Dense(units=32, activation='relu'), # second hidden layer\n",
        "    Dense(units=2, activation='softmax') # output layer with a softmax\n",
        "])"
      ],
      "metadata": {
        "id": "0VyUud-JhXo3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIAV7BIsiEFW",
        "outputId": "eb7c4db9-4f98-4dfb-d0bb-4d5719995fcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                32        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 642 (2.51 KB)\n",
            "Trainable params: 642 (2.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import metrics\n",
        "#compile our model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )"
      ],
      "metadata": {
        "id": "3VjhrLRGiGaY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import VERBOSE\n",
        "# Fitting the with our data\n",
        "model.fit(x=scaled_train_samples,\n",
        "          y=train_label, batch_size=10,\n",
        "          epochs=30,\n",
        "          shuffle=True,\n",
        "          verbose=2\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyUUC_Zo-Yrw",
        "outputId": "6c6f54fe-091f-4549-ceb7-b3b9458ddea9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "210/210 - 7s - loss: 0.7016 - accuracy: 0.5005 - 7s/epoch - 32ms/step\n",
            "Epoch 2/30\n",
            "210/210 - 0s - loss: 0.6774 - accuracy: 0.6571 - 459ms/epoch - 2ms/step\n",
            "Epoch 3/30\n",
            "210/210 - 0s - loss: 0.6529 - accuracy: 0.7343 - 466ms/epoch - 2ms/step\n",
            "Epoch 4/30\n",
            "210/210 - 1s - loss: 0.6243 - accuracy: 0.7981 - 617ms/epoch - 3ms/step\n",
            "Epoch 5/30\n",
            "210/210 - 1s - loss: 0.5950 - accuracy: 0.8048 - 648ms/epoch - 3ms/step\n",
            "Epoch 6/30\n",
            "210/210 - 1s - loss: 0.5639 - accuracy: 0.8214 - 650ms/epoch - 3ms/step\n",
            "Epoch 7/30\n",
            "210/210 - 1s - loss: 0.5313 - accuracy: 0.8376 - 611ms/epoch - 3ms/step\n",
            "Epoch 8/30\n",
            "210/210 - 0s - loss: 0.4986 - accuracy: 0.8548 - 445ms/epoch - 2ms/step\n",
            "Epoch 9/30\n",
            "210/210 - 0s - loss: 0.4674 - accuracy: 0.8619 - 452ms/epoch - 2ms/step\n",
            "Epoch 10/30\n",
            "210/210 - 0s - loss: 0.4385 - accuracy: 0.8752 - 438ms/epoch - 2ms/step\n",
            "Epoch 11/30\n",
            "210/210 - 0s - loss: 0.4116 - accuracy: 0.8829 - 433ms/epoch - 2ms/step\n",
            "Epoch 12/30\n",
            "210/210 - 0s - loss: 0.3870 - accuracy: 0.8933 - 458ms/epoch - 2ms/step\n",
            "Epoch 13/30\n",
            "210/210 - 0s - loss: 0.3658 - accuracy: 0.9024 - 434ms/epoch - 2ms/step\n",
            "Epoch 14/30\n",
            "210/210 - 0s - loss: 0.3477 - accuracy: 0.9076 - 482ms/epoch - 2ms/step\n",
            "Epoch 15/30\n",
            "210/210 - 0s - loss: 0.3325 - accuracy: 0.9105 - 440ms/epoch - 2ms/step\n",
            "Epoch 16/30\n",
            "210/210 - 0s - loss: 0.3201 - accuracy: 0.9176 - 432ms/epoch - 2ms/step\n",
            "Epoch 17/30\n",
            "210/210 - 0s - loss: 0.3097 - accuracy: 0.9186 - 488ms/epoch - 2ms/step\n",
            "Epoch 18/30\n",
            "210/210 - 0s - loss: 0.3012 - accuracy: 0.9214 - 444ms/epoch - 2ms/step\n",
            "Epoch 19/30\n",
            "210/210 - 0s - loss: 0.2941 - accuracy: 0.9262 - 446ms/epoch - 2ms/step\n",
            "Epoch 20/30\n",
            "210/210 - 0s - loss: 0.2882 - accuracy: 0.9267 - 466ms/epoch - 2ms/step\n",
            "Epoch 21/30\n",
            "210/210 - 0s - loss: 0.2834 - accuracy: 0.9271 - 451ms/epoch - 2ms/step\n",
            "Epoch 22/30\n",
            "210/210 - 0s - loss: 0.2793 - accuracy: 0.9329 - 454ms/epoch - 2ms/step\n",
            "Epoch 23/30\n",
            "210/210 - 0s - loss: 0.2760 - accuracy: 0.9286 - 469ms/epoch - 2ms/step\n",
            "Epoch 24/30\n",
            "210/210 - 0s - loss: 0.2728 - accuracy: 0.9319 - 453ms/epoch - 2ms/step\n",
            "Epoch 25/30\n",
            "210/210 - 0s - loss: 0.2702 - accuracy: 0.9352 - 458ms/epoch - 2ms/step\n",
            "Epoch 26/30\n",
            "210/210 - 0s - loss: 0.2680 - accuracy: 0.9319 - 429ms/epoch - 2ms/step\n",
            "Epoch 27/30\n",
            "210/210 - 0s - loss: 0.2660 - accuracy: 0.9357 - 454ms/epoch - 2ms/step\n",
            "Epoch 28/30\n",
            "210/210 - 0s - loss: 0.2642 - accuracy: 0.9348 - 452ms/epoch - 2ms/step\n",
            "Epoch 29/30\n",
            "210/210 - 1s - loss: 0.2628 - accuracy: 0.9357 - 550ms/epoch - 3ms/step\n",
            "Epoch 30/30\n",
            "210/210 - 1s - loss: 0.2612 - accuracy: 0.9352 - 613ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x796b37f85630>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0nvGL48i0cj",
        "outputId": "91ab2c3d-5e17-459d-da5f-83062b22835d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Predictions on new Data**"
      ],
      "metadata": {
        "id": "jpNZ1mg8si5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new data (scaled in the same way as the training data)\n",
        "new_data = [30, 54, 100, 40, 70, 99, 20, 14, 12]\n",
        "\n",
        "# convert new data to numpy array\n",
        "new_data = np.array(new_data)\n",
        "\n",
        "# scale and transform our data\n",
        "scaller = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "scaled_test_data = scaller.fit_transform(new_data.reshape(-1, 1))\n",
        "\n",
        "print(\"New Data:\", scaled_test_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(scaled_test_data)\n",
        "\n",
        "# The predictions will be in the form of probabilities for each class\n",
        "# In your case, it's binary classification (side effect or no side effect), so you'll get two probabilities for each input\n",
        "# You can interpret the results based on the threshold (e.g., if probability > 0.5, classify as side effect)\n",
        "\n",
        "# If you want to get the class label directly, you can use argmax\n",
        "class_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# class_labels will contain the predicted labels (0 or 1) for your new data\n",
        "print(class_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjE0jOX9i2M0",
        "outputId": "a0ff296b-9f18-4a6b-f820-f700a9c6790f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Data: [[0.20454545]\n",
            " [0.47727273]\n",
            " [1.        ]\n",
            " [0.31818182]\n",
            " [0.65909091]\n",
            " [0.98863636]\n",
            " [0.09090909]\n",
            " [0.02272727]\n",
            " [0.        ]]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[0 0 1 0 1 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCNqtX2axK5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cCOH3EuVtWAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}